# ACM-Net
## introdution
### ç›®å‰å­˜åœ¨çš„
**formulate  the  weakly-supervised  temporal  actionlocalization  as  a  video  recognition  problem  and  introducea  foreground-background  separation  attention  mechanism  toconstruct  video-level  features:**
*.  Nguyen,  T.  Liu,  G.  Prasad,  and  B.  Han,  â€œWeakly  supervised  actionlocalization by sparse temporal pooling network,â€ inCVPR, 2018.
[12]  K. Min and J. J. Corso, â€œAdversarial background-aware loss for weakly-supervised temporal activity localization,â€ inECCV, 2020.
[13]  T. Yu, Z. Ren, Y. Li, E. Yan, N. Xu, and J. Yuan, â€œTemporal structuremining for weakly supervised action detection,â€ inICCV, 2019.
[14]  P.  X.  Nguyen,  D.  Ramanan,  and  C.  C.  Fowlkes,  â€œWeakly-supervisedaction localization with background modeling,â€ inICCV, 2019.*
**MIL:**  
Treat  the  entire  untrimmed  video  as  a  bagcontaining both positive and negative instances, i.e. foregroundaction  instances  frames  and  background  non-action  frames  
These methods first apply a classifier to obtain temporal-pointclass  activation  sequence  (CAS)  and  then  employ  a  top-kmechanism to aggregate the video-level classification scores
*[19]  S. Paul, S. Roy, and A. K. Roy-Chowdhury, â€œW-talc: Weakly-supervisedtemporal activity localization and classification,â€ inECCV, 2018.
[20]  Y. Yuan, Y. Lyu, X. Shen, I. W. Tsang, and D.-Y. Yeung, â€œMarginalizedaverage  attentional  network  for  weakly-supervised  learning,â€  inICLR,2019.
[21]  P.  Lee,  Y.  Uh,  and  H.  Byun,  â€œBackground  suppression  network  forweakly-supervised temporal action localization,â€ inAAAI, 2020.
[22]  Z.  Luo,  D.  Guillory,  B.  Shi,  W.  Ke,  F.  Wan,  T.  Darrell,  and  H.  Xu,â€œWeakly-supervised  action  localization  with  expectation-maximizationmulti-instance learning,â€ inECCV, 2020*
### å­˜åœ¨é—®é¢˜
éš¾ä»¥åŒºåˆ†contextä¸action,
### METHODOLOGY
**freature extraction:**RGB ä¸å…‰æµ I3D
**Feature Embedding:**å·ç§¯ç‰¹å¾F  X=ReLU(Conv(F,theta))
**Action Class Activation Modeling:** CAS=MLP(X,theta)
**Action Context Attention Modeling** ğ´=Softmax(Conv(ğ‘‹,ğœƒğ‘ğ‘¡ğ‘¡ÂºÂº)) A={(att_ins att_con att_bak} CAS_ins=att_int*CAS åŒç† CAS_con CASâ€”â€”bakéƒ½èƒ½å¾—åˆ°
**Multiple-Instance  Learning**top-k å–å„ä¸ªbranchçš„CAS kæœ€å¤§çš„å¹³å‡ï¼Œç”¨softmaxè®¡ç®—p åˆ†åˆ«ä¸ºp_ins p_con p_bak
*top-k è®¡ç®—æ¯ç±»çš„CASçš„æœ€å¤§kå€¼çš„å¹³å‡ ä½œä¸ºè¯¥ç±»å‡ºç°çš„æ¦‚ç‡è¡¨ç¤º*
æ¯ä¸€ä¸ªåˆ†æ”¯çš„æŸå¤±ï¼ŒL=-ylog(p) y_ins=[yn=1,y(c+1)=0] y_con=[yn=1 y(c+1)=1] y_bak=[yn=0 y(c+1)=1]
L_cls=Lins+Lcon+Lbak
**Optimization Objects** L=L_cls+L_add   L_add=L_gui+L_feat+L_spa
L_gui ä¼˜åŒ–ç‰‡æ®µçº§åˆ«çš„ç»“æœ ä½¿ç”¨pä¸att_ins att_insæ˜¯ç‰‡æ®µçº§æ³¨æ„åŠ›ï¼Œpæ˜¯ä¸Šæ–‡å¾—å‡ºçš„
L_feat ä¸ºäº†featureæ›´åŠ çš„distinguishable
