[pytorch多卡训练](https://zhuanlan.zhihu.com/p/102697821) 
[Pytorch多机多卡分布式训练](https://zhuanlan.zhihu.com/p/68717029) 
**大概而言，首先添加如下代码，指定一下代码运行在gpu的物理卡号**  
```(python)
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "2, 3"
```
或者是
```(python)
os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID#调用其他文件的代码
GPU_ID='2,3'
```
甚至在命令行使用如下语句也可以
`CUDA_VISBLE_DEVICES=2,3 python train.py`(如果代码中有os.environ 则优先os.environ)  
**其次，要使用如下语句**
```(python)
net.cuda()
device_ids = [0, 1]#注意此处编号为逻辑编号，pytorch要求从devices[0]开始
net = torch.nn.DataParallel(net, device_ids=device_ids)
```
