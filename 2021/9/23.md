torch.cat  
for example  
```python
>>> x = torch.randn(2, 3)
>>> x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])
```
[变分自编码器（VAE）的代码理解](https://blog.csdn.net/B_C_Wang/article/details/74908408?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.no_search_link&spm=1001.2101.3001.4242)  
> 变分自编码器(VAE) 常常和GAN一同被提及，它们都属于生成模型，采用少量样本即可实现训练。步骤如下（结合后面的模型结构进行分析）:
>1. encoder或判别器将输入的n维数据转换为2个m维数据，其中第一个视作**m个高斯分布的均值，第二个视作**m个高斯分布方差的对数
>2. 根据上述2个m维的数据，生成m维的服从高斯分布的随机数
>3. 高斯分布先生成(None, m)的正态分布矩阵a，然后out = mean + exp(log_var/2) * a 输出encoder层的结果，注意是先生成N(0,1)再乘，而不是直接生成N(mean,log_var)，这样只涉及线性操作   

[VAE实现](https://blog.csdn.net/jackytintin/article/details/53641885)  

torch.randn 生成一个分布为N(0,1)随机tensor  

##pip install -r requirements.txt 到指定环境的方法  
第一步 创建环境  
`>>conda create -n justfun`  
第二步 激活环境  
`>>conda activate justfun`  
第三步 在环境下安装pip(important)   
`>>conda install pip`  
第四步 查看pip所在的路径（在环境路径下）  
```
>>which pip
/home/yuzikang/anaconda3/envs/justfun/bin/pip
```  
第五步   
`pip install -r requirements.txt`  
看到/home/yuzikang/anaconda3/envs/justfun/lib/python3.9/site-packages （envs/justfun）成功   
可以运行 `conda list` 查看安装成功否   
